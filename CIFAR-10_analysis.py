# -*- coding: utf-8 -*-
"""MidProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vfK8Y2ECCURGaCGL4GLy01gOR15eavxV

# Applied Neural Networks Mid-Project

Due Tuesday 11am, March 31, 2020

Set up imports and download [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
from tensorflow import keras

(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()

"""**Question 1**<br/>
Explore and preprocess the CIFAR-10 datset. There should be 50 000 images in your training set and 10 000 images in your testing set. Each image is 32x32 pixels and each pixel has an RGB 3-vector. There are ten different classes.
"""

X_train_full.shape

y_train_full.shape

X_test.shape

y_train_full[:15]

X_train_full[0]

import matplotlib.pyplot as plt
plt.imshow(X_train_full[0], cmap="binary")

X_train_full = X_train_full/255
X_test = X_test/255

X_train_full[0]

plt.imshow(X_train_full[0],cmap='binary')

index = 6
plt.imshow(X_train_full[index],cmap='binary')
plt.title(y_train_full[index])

classes = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog',
           'Frog', 'Horse', 'Ship', 'Truck']

X_train_full = X_train_full.reshape(-1,3072)
X_test = X_test.reshape(-1,3072)

"""**Question 2**<br/>
Train an ANN with 4 hidden layers of 500 neurons each. Use an early stopping callback with a patience of 10 epochs. Show the model summary and architecture plot. Give the accuracy. Make sure to train the network on validation data and reserve your test data for evaluation. Plot the accuracy vs epoch. 

*Hint:* you might find <br/>
keras.layers.Flatten(input_shape=(32, 32, 3)) <br/>
useful as it will flatten your input without you needing to reshape the data

"""

from sklearn.model_selection import train_test_split
X_train_tr, X_train_v, y_train_tr, y_train_v = train_test_split(X_train_full,
                                                                y_train_full,
                                                                test_size=1/10,
                                                                random_state=42)
X_train_tr.shape

X_train_v.shape

model_1 = keras.models.Sequential()
model_1.add(keras.layers.Dense(500,activation="relu",input_shape=(3072,), 
                             name="1st_Hidden_Layer"))
model_1.add(keras.layers.Dense(500,activation="relu",
                             name="2nd_Hidden_Layer"))
model_1.add(keras.layers.Dense(500,activation="relu",
                             name="3rd_Hidden_Layer"))
model_1.add(keras.layers.Dense(500,activation="relu",
                             name="4th_Hidden_Layer"))
model_1.add(keras.layers.Dense(10,activation="softmax",
                             name="Output_Layer"))

model_1.summary()

keras.utils.plot_model(model_1,show_shapes=True)

checkpoint_cb = keras.callbacks.ModelCheckpoint("temp_model.tf", save_best_only=True)

model_1.compile(loss="sparse_categorical_crossentropy",optimizer="sgd",
              metrics=["accuracy"])
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, 
                                                  restore_best_weights=True,
                                                  verbose=1)
history_01 = model_1.fit(X_train_tr,y_train_tr, epochs=100, batch_size=32,
                    validation_data=(X_train_v,y_train_v),
                    callbacks=[checkpoint_cb, early_stopping_cb])

history_01.params

history_01.epoch

history_01.history

import pandas as pd
pd.DataFrame(history_01.history).plot(figsize=(10,7))

model_1.evaluate(X_test,y_test)

"""**Question 3**<br/>
Use RandomizedSearchCV to tune your hyperparameters (you will need to use the Subclassing API). Check the following parameters: <br/>
number of hidden layers = [2, 4, 6, 8, 10] <br/>
number of neurons = [100, 400, 700, 1000, 1300] <br/>
learning rate = [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2] <br/>

Which parameters performed best? Discuss.
"""

import numpy as np
keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

# Creating Subclass

class new_model(keras.models.Model):  # in python superclass is in ()
  def __init__(self, units=500, activation="relu", n_hidden=4, **kwargs):  # constructor (cannot overload in python), set default with =
    super().__init__(**kwargs)       # super() function will make the child class inherit all the methods and properties from its parent
    self.hidden1 = keras.layers.Dense(units, activation=activation)
    self.hidden2 = keras.layers.Dense(units, activation=activation)
    self.output_main = keras.layers.Dense(10)
    self.n_hidden = n_hidden
  def call(self, inputs):  # keras uses for "forward" pass
    x = inputs
    x = self.hidden1(x)
    for layer in range(self.n_hidden):
      x = self.hidden2(x)
    output_main = self.output_main(x)
    return output_main

# function to build neural net
def build_model(n_hidden=1, n_neurons=500, learning_rate=3e-3): # setting up default values
  model_cus = new_model(units=n_neurons,activation="relu", n_hidden=n_hidden)
  model_cus.compile(loss="sparse_categorical_crossentropy",
                optimizer=keras.optimizers.SGD(lr=learning_rate),
                metrics=["accuracy"])
  return model_cus

keras_class = keras.wrappers.scikit_learn.KerasClassifier(build_model)

keras_class.fit(X_train_tr,y_train_tr, epochs=10, 
              validation_data=(X_train_v,y_train_v),
              callbacks=[keras.callbacks.EarlyStopping(patience=5)])

# RandomizedSearchCV parameters

from sklearn.model_selection import RandomizedSearchCV
param_distribs = {
    "n_hidden": [2, 4, 6, 8, 10],
    "n_neurons": [100, 400, 700, 1000, 1300],
    "learning_rate": [1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2]
}

rnd_search_cv = RandomizedSearchCV(keras_class, param_distribs, n_iter=10, cv=3, verbose=1)
rnd_search_cv.fit(X_train_tr,y_train_tr, epochs=100, validation_data=(X_train_v,y_train_v),
                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])

rnd_search_cv.best_params_

rnd_search_cv.best_score_

rnd_search_cv.best_estimator_

"""## The best score found is 0.1148 which is very low and best parameter found at learning rate 0.001 and the number of hidden layer is 10 and number of neuron in each hidden layer is 1000. RandomsearchCV do not work with all combination of the parameter like gridsearchCV. That could be a reason for getting low accuracy.

**Question 4**<br/>
Do at least one more thing to change your neural network.<br/>
Describe the best neural network you found in this project. Include the plot of accuracy vs epoch, and plots of five typical misclassifications. Give the accuracy of this best model for each class (is it better/worse at classifying some classes?).

## I am building the neural network with functional API with 2 hidden layer which consists of 500 neuron each.
"""

keras.backend.clear_session()
np.random.seed(42)
tf.random.set_seed(42)

input_ = keras.layers.Input(shape=(3072,))
hidden1 = keras.layers.Dense(500,activation="relu")(input_)
hidden2 = keras.layers.Dense(500,activation="relu")(hidden1)
concat = keras.layers.concatenate([input_,hidden2])
output = keras.layers.Dense(10,activation="softmax")(concat)
model_2 = keras.models.Model(inputs=[input_], outputs=[output])

model_2.summary()

keras.utils.plot_model(model_2,show_shapes=True)

checkpoint_cb = keras.callbacks.ModelCheckpoint("temp_model.tf", save_best_only=True)

model_2.compile(loss="sparse_categorical_crossentropy",optimizer="sgd",
              metrics=["accuracy"])
early_stopping_cb = keras.callbacks.EarlyStopping(patience=10, 
                                                  restore_best_weights=True,
                                                  verbose=1)
history_2 = model_2.fit(X_train_tr,y_train_tr, epochs=30, batch_size=32,
                    validation_data=(X_train_v,y_train_v),
                    callbacks=[checkpoint_cb, early_stopping_cb])

pd.DataFrame(history_2.history).plot(figsize=(10,7))

model_2.evaluate(X_test,y_test)

"""## The best model I found in this project by sequential API which is model_1 and the test accuracy of this model is 0.50."""

# Plot of Accuracy vs Epoch (from best model)

pd.DataFrame(history_01.history).plot(figsize=(10,7))

# Display misclassified images from best model
import matplotlib.pyplot as plt

predictions = model_1.predict_classes(X_test[:])
misclassifiedIndexes = []
index = 0
for target, prediction in zip(y_test, predictions):
  if target != prediction:
    misclassifiedIndexes.append(index)
  index += 1

misclassifiedIndexes

plt.figure(figsize=(20,4))
for index in range(5):
  plt.subplot(1,5,index+1)
  plt.imshow(X_test[misclassifiedIndexes[index]].reshape(32,32,3))
  plt.axis('off')
  plt.title(f'Target: {y_test[misclassifiedIndexes[index]]}    Predicted: {predictions[misclassifiedIndexes[index]]}', fontsize=14)

plt.show()

# Accuracy for each class

from sklearn.metrics import classification_report
import numpy as np

y_pred = model_1.predict_classes(X_test)
print(classification_report(y_test, y_pred))

"""## If we see the individual class accuracy, then we can see the worse accuracy for class 3 where we are getting the best accuracy for class 8.

Note that convolution neural networks will be better predictors on this dataset than ANNs. We will revisit this in the final project (for undergraduates).
"""